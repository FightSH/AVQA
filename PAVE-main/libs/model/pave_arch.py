# This script holds the implementation of the PAVE model.
# è¯¥è„šæœ¬åŒ…å«äº† PAVE æ¨¡å‹çš„å®ç°ã€‚

from abc import ABC, abstractmethod

import torch
import torch.nn as nn

import math
import ipdb
import logging
from einops import rearrange, repeat

from .multimodal_encoder.builder import build_temporal_aggregator, build_video_tower, build_vision_tower
from .multimodal_projector.builder import build_vision_projector

from libs.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from libs.mm_utils import get_anyres_image_grid_shape, split_list_lengths
from libs.utils.train_utils import rank0_print


def get_weight(weights, keyword):
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºä»é¢„è®­ç»ƒæ¨¡å‹çš„ state_dict ä¸­æå–ç‰¹å®šæ¨¡å—çš„æƒé‡ã€‚"""
    # {k.split(keyword + ".")[1]: v for k, v in weights.items() if keyword in k}
    # éå†æƒé‡å­—å…¸ï¼Œå¦‚æœé”®ï¼ˆkï¼‰ä¸­åŒ…å«æŒ‡å®šçš„å…³é”®å­—ï¼ˆkeywordï¼‰ï¼Œ
    # å°±å°†å…³é”®å­—åŠå…¶å‰é¢çš„éƒ¨åˆ†ï¼ˆå¦‚ 'model.vision_tower.'ï¼‰å»æ‰ï¼Œ
    # ç„¶åå°†å‰©ä½™çš„é”®å’Œå¯¹åº”çš„å€¼ï¼ˆvï¼‰å­˜å…¥æ–°çš„å­—å…¸ä¸­ã€‚
    return {k.split(keyword + ".")[1]: v for k, v in weights.items() if keyword in k}


class PAVEMetaModel:
    """
    PAVE æ¨¡å‹çš„å…ƒç±»ï¼ˆMeta Modelï¼‰ã€‚
    è¿™ä¸ªç±»ä¸»è¦è´Ÿè´£å®šä¹‰å’Œåˆå§‹åŒ–æ¨¡å‹çš„å„ä¸ªç»„ä»¶ï¼Œå¦‚è§†è§‰å¡”ã€è§†é¢‘å¡”ç­‰ã€‚
    å®ƒä¸åŒ…å«å®Œæ•´çš„ forward é€»è¾‘ï¼Œæ›´åƒæ˜¯ä¸€ä¸ªæ¨¡å‹ç»“æ„çš„å®¹å™¨ã€‚
    """

    def __init__(self, config):
        super(PAVEMetaModel, self).__init__(config)
        print("å¼€å§‹åˆå§‹åŒ–PAVEå…ƒæ¨¡å‹")

        # The Fast-Path (å¿«é€Ÿè·¯å¾„ï¼Œé€šå¸¸ç”¨äºå¤„ç†è§†é¢‘æµ)
        if hasattr(config, "mm_video_tower"):
            print("æ£€æµ‹åˆ°mm_video_toweré…ç½®ï¼Œåˆå§‹åŒ–å¿«é€Ÿè·¯å¾„ç»„ä»¶")
            #build the encoder (MAE + diffusion)
            # æ„å»ºè§†é¢‘ç¼–ç å™¨ï¼ˆä¾‹å¦‚ï¼Œç»“åˆäº† MAE å’Œ diffusion çš„æ¨¡å‹ï¼‰
            print("æ„å»ºè§†é¢‘å¡”ï¼Œdelay_load=True")
            self.video_tower = build_video_tower(config, delay_load=True) # delay_load=True è¡¨ç¤ºç¨åæ‰ä¼šçœŸæ­£åŠ è½½æƒé‡
            #TODO: build the compresser (SSM) We may not need to instantiate it now
            #TODO: æ„å»ºå‹ç¼©å™¨ (SSM)ï¼Œè¿™é‡Œå¯èƒ½æš‚æ—¶ä¸éœ€è¦å®ä¾‹åŒ–
            # æ„å»ºæ—¶é—´èšåˆå™¨ï¼Œç”¨äºæ²¿æ—¶é—´ç»´åº¦èšåˆè§†é¢‘å¸§ç‰¹å¾
            print("æ„å»ºæ—¶é—´èšåˆå™¨")
            self.temporal_aggregator = build_temporal_aggregator(config)  # ç¬¬ä¸€æ¬¡æ„å»º
        else:
            print("æœªæ£€æµ‹åˆ°mm_video_toweré…ç½®ï¼Œè·³è¿‡å¿«é€Ÿè·¯å¾„åˆå§‹åŒ–")

        # The Slow-Path (æ…¢é€Ÿè·¯å¾„ï¼Œé€šå¸¸ç”¨äºå¤„ç†é™æ€å›¾åƒæˆ–å…³é”®å¸§)
        if hasattr(config, "mm_vision_tower"):
            print("æ£€æµ‹åˆ°mm_vision_toweré…ç½®ï¼Œåˆå§‹åŒ–æ…¢é€Ÿè·¯å¾„ç»„ä»¶")
            # delay_load è¡¨ç¤ºæ˜¯å¦å»¶è¿ŸåŠ è½½æ¨¡å‹æƒé‡ï¼Œé»˜è®¤ä¸º False
            delay_load = getattr(config, "delay_load", False)
            print(f"æ„å»ºè§†è§‰å¡”ï¼Œdelay_load={delay_load}")
            # æ„å»ºè§†è§‰å¡”ï¼ˆç”¨äºå¤„ç†å›¾åƒï¼‰
            self.vision_tower = build_vision_tower(config, delay_load=delay_load)
            # self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)
            # æ„å»ºå¤šæ¨¡æ€æŠ•å½±å™¨ï¼Œå°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ°ä¸æ–‡æœ¬ç‰¹å¾ç›¸åŒçš„ç»´åº¦ç©ºé—´
            print("æ„å»ºå¤šæ¨¡æ€æŠ•å½±å™¨")
            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)

            # å¦‚æœ patch åˆå¹¶ç±»å‹åŒ…å« "unpad"ï¼Œåˆ™éœ€è¦ä¸€ä¸ªé¢å¤–çš„å¯å­¦ä¹ å‚æ•°ä½œä¸ºå›¾åƒæ¢è¡Œç¬¦
            mm_patch_merge_type = getattr(config, "mm_patch_merge_type", "")
            if "unpad" in mm_patch_merge_type:
                print(f"æ£€æµ‹åˆ°unpadç±»å‹patchåˆå¹¶({mm_patch_merge_type})ï¼Œåˆå§‹åŒ–image_newlineå‚æ•°")
                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))
            else:
                print(f"patchåˆå¹¶ç±»å‹ä¸º{mm_patch_merge_type}ï¼Œä¸éœ€è¦image_newlineå‚æ•°")
        else:
            print("æœªæ£€æµ‹åˆ°mm_vision_toweré…ç½®ï¼Œè·³è¿‡æ…¢é€Ÿè·¯å¾„åˆå§‹åŒ–")
        
        print("PAVEå…ƒæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def get_video_tower(self):
        """è·å–è§†é¢‘å¡”å®ä¾‹ã€‚å¤„ç† FSDPï¼ˆFully Sharded Data Parallelï¼‰å¯èƒ½å°†å…¶åŒ…è£…åœ¨åˆ—è¡¨ä¸­çš„æƒ…å†µã€‚"""
        video_tower = getattr(self, 'video_tower', None)
        if type(video_tower) is list:
            print("è§†é¢‘å¡”è¢«FSDPåŒ…è£…åœ¨åˆ—è¡¨ä¸­ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ ")
            video_tower = video_tower[0]
        return video_tower

    def get_vision_tower(self):
        """è·å–è§†è§‰å¡”å®ä¾‹ã€‚åŒæ ·å¤„ç† FSDP çš„æƒ…å†µã€‚"""
        vision_tower = getattr(self, "vision_tower", None)
        if type(vision_tower) is list:
            print("è§†è§‰å¡”è¢«FSDPåŒ…è£…åœ¨åˆ—è¡¨ä¸­ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ ")
            vision_tower = vision_tower[0]
        return vision_tower

    def initialize_vision_modules(self, model_args, fsdp=None):
        """
        åˆå§‹åŒ–æ¨¡å‹çš„è§†è§‰ç›¸å…³æ¨¡å—ã€‚
        è¿™ä¸ªå‡½æ•°è´Ÿè´£æ„å»ºæ¨¡å—ã€åŠ è½½é¢„è®­ç»ƒæƒé‡ç­‰ã€‚
        """
        print("å¼€å§‹åˆå§‹åŒ–è§†è§‰æ¨¡å—")
        
        #### The Fast-path Init ### (å¿«é€Ÿè·¯å¾„åˆå§‹åŒ–)
        video_tower = model_args.video_tower # ä»æ¨¡å‹å‚æ•°ä¸­è·å–è§†é¢‘å¡”çš„é…ç½®æˆ–è·¯å¾„
        pretrain_temporal_aggregator = model_args.pretrain_temporal_aggregator # è·å–é¢„è®­ç»ƒçš„æ—¶é—´èšåˆå™¨æƒé‡è·¯å¾„
        self.config.mm_video_tower = video_tower # æ›´æ–°æ¨¡å‹é…ç½®
        
        print(f"å¿«é€Ÿè·¯å¾„é…ç½®: video_tower={video_tower}, pretrain_temporal_aggregator={pretrain_temporal_aggregator}")

        ### init and load the pretrained video tower backbone (åˆå§‹åŒ–å¹¶åŠ è½½é¢„è®­ç»ƒçš„è§†é¢‘å¡”éª¨å¹²ç½‘ç»œ)
        if self.get_video_tower() is None:
            print("è§†é¢‘å¡”æœªåˆå§‹åŒ–ï¼Œå¼€å§‹æ„å»º")
            # å¦‚æœè§†é¢‘å¡”å°šæœªåˆå§‹åŒ–ï¼Œåˆ™æ ¹æ®é…ç½®æ„å»ºå®ƒ
            video_tower = build_video_tower(model_args)

            if fsdp is not None and len(fsdp) > 0:
                print("ä½¿ç”¨FSDPï¼Œå°†è§†é¢‘å¡”åŒ…è£…åœ¨åˆ—è¡¨ä¸­")
                # å¦‚æœä½¿ç”¨ FSDPï¼Œå°†æ¨¡å‹åŒ…è£…åœ¨åˆ—è¡¨ä¸­
                self.video_tower = [video_tower]
            else:
                print("ä¸ä½¿ç”¨FSDPï¼Œç›´æ¥è®¾ç½®è§†é¢‘å¡”")
                self.video_tower = video_tower
        else:
            print("è§†é¢‘å¡”å·²å­˜åœ¨ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡")
            # å¦‚æœè§†é¢‘å¡”å·²å­˜åœ¨ï¼Œåˆ™è·å–å…¶å®ä¾‹
            if fsdp is not None and len(fsdp) > 0:
                video_tower = self.video_tower[0]
            else:
                video_tower = self.video_tower
            # load the checkpoint (for step 1 and step 2)
            # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆç”¨äºè®­ç»ƒçš„ç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥ï¼‰
            video_tower.load_model()

        ### build the temporal aggregator again (å†æ¬¡æ„å»ºæ—¶é—´èšåˆå™¨)
        self.config.temporal_aggregator_type = getattr(model_args, 'temporal_aggregator_type', 'ssm')
        print(f"æ—¶é—´èšåˆå™¨ç±»å‹: {self.config.temporal_aggregator_type}")
        
        if getattr(self, 'temporal_aggregator', None) is None:
            print("æ—¶é—´èšåˆå™¨æœªåˆå§‹åŒ–ï¼Œå¼€å§‹æ„å»º")
            # å¦‚æœæ—¶é—´èšåˆå™¨æœªåˆå§‹åŒ–ï¼Œåˆ™æ„å»ºå®ƒ
            self.temporal_aggregator = build_temporal_aggregator(model_args)  # ç¬¬äºŒæ¬¡æ„å»º
        else:
            print("æ—¶é—´èšåˆå™¨å·²å­˜åœ¨ï¼Œè§£å†»å‚æ•°ä»¥è¿›è¡Œè®­ç»ƒ")
            # In case it is frozen by LoRA
            # å¦‚æœå®ƒå› ä¸º LoRA ç­‰æŠ€æœ¯è¢«å†»ç»“äº†ï¼Œéœ€è¦è§£å†»ä»¥è¿›è¡Œè®­ç»ƒ
            for p in self.temporal_aggregator.parameters():
                p.requires_grad = True

        # load the existing temporal aggregator checkpoint (for step 2 training)
        # åŠ è½½å·²æœ‰çš„æ—¶é—´èšåˆå™¨æƒé‡ï¼ˆç”¨äºè®­ç»ƒçš„ç¬¬äºŒæ­¥ï¼‰
        if pretrain_temporal_aggregator is not None:
            print(f"åŠ è½½é¢„è®­ç»ƒæ—¶é—´èšåˆå™¨æƒé‡: {pretrain_temporal_aggregator}")
            temporal_aggregator_weights = torch.load(pretrain_temporal_aggregator, map_location='cpu')
            # ipdb.set_trace()
            def get_w(weights, keyword, module):
                """ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”¨äºåŠ è½½æƒé‡å¹¶å¤„ç†å¯èƒ½ä¸åŒ¹é…çš„é”®ã€‚"""
                print(f"æå–å…³é”®å­—'{keyword}'çš„æƒé‡")
                # æå–ç‰¹å®šæ¨¡å—çš„æƒé‡
                temp = {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}
                
                # handle the running means (å¤„ç† state_dict ä¸­çš„ running_mean/var ç­‰)
                module_state_dict = module.state_dict()
                # check the loading (æ£€æŸ¥åŠ è½½æƒ…å†µ)
                if len(module_state_dict) >= len(temp):
                    # æ‰¾å‡ºå½“å‰æ¨¡å—æœ‰ä½†åŠ è½½æƒé‡ä¸­æ²¡æœ‰çš„é”®ï¼ˆå¯èƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–ï¼‰
                    missed_key = [ele for ele in module_state_dict if ele not in temp]
                    
                    if missed_key:
                        print(f"å‘ç°ç¼ºå¤±çš„æƒé‡é”®: {missed_key}")
                    
                    # hacky way to get the data type (ç”¨ä¸€ç§å–å·§çš„æ–¹å¼è·å–æƒé‡çš„æ•°æ®ç±»å‹)
                    data_type = None
                    for key in temp:
                        data_type = temp[key].dtype
                        break
                    
                    # å¯¹ç¼ºå¤±çš„é”®ï¼Œä½¿ç”¨å½“å‰æ¨¡å—çš„åˆå§‹å€¼ï¼Œå¹¶è½¬æ¢åˆ°æ­£ç¡®çš„ç±»å‹
                    for key in missed_key:
                        print(f'å‚æ•° {key} è¢«é‡æ–°åˆå§‹åŒ–')
                        temp[key] = module_state_dict[key].to(data_type)  
                return temp
            
            # åŠ è½½æ—¶é—´èšåˆå™¨çš„æƒé‡
            loaded_weights = get_w(temporal_aggregator_weights, 'temporal_aggregator', self.temporal_aggregator)
            self.temporal_aggregator.load_state_dict(loaded_weights)
            print("æ—¶é—´èšåˆå™¨æƒé‡åŠ è½½å®Œæˆ")
        else:
            print("æœªæä¾›é¢„è®­ç»ƒæ—¶é—´èšåˆå™¨æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

        ### The Slow-path Init. This section should be skipped ### (æ…¢é€Ÿè·¯å¾„åˆå§‹åŒ–ã€‚æ­¤éƒ¨åˆ†åº”è¢«è·³è¿‡)
        # ipdb.set_trace() # check this part is skipped (æ£€æŸ¥è¿™éƒ¨åˆ†ä»£ç æ˜¯å¦æŒ‰é¢„æœŸè¢«è·³è¿‡)
        if hasattr(model_args, 'vision_tower') and self.get_vision_tower() is None:
            print('é‡æ–°åŠ è½½/é‡æ–°åˆå§‹åŒ–æ…¢é€Ÿè·¯å¾„ä¸­çš„vision_tower')
            # è¿™éƒ¨åˆ†é€»è¾‘ç”¨äºåˆå§‹åŒ–æ…¢é€Ÿè·¯å¾„ï¼ˆå›¾åƒå¤„ç†ï¼‰ï¼Œä½†åœ¨å½“å‰è®¾ç½®ä¸‹å¯èƒ½ä¸ä¼šæ‰§è¡Œ
            vision_tower = model_args.vision_tower
            mm_vision_select_layer = model_args.mm_vision_select_layer
            mm_vision_select_feature = model_args.mm_vision_select_feature
            pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter     # check whether we train the adapter in the step1 training
            mm_patch_merge_type = model_args.mm_patch_merge_type
            pretrain_vision_modules = model_args.pretrain_vision_modules     # Considering the case we directly load the whole vision module from other model
            # ipdb.set_trace() # check pretrain_vision_modules

            self.config.mm_vision_tower = vision_tower
            self.config.vision_tower_pretrained = getattr(model_args, "vision_tower_pretrained", "")

            # Load the vision backbone (Image Backbone)
            # åŠ è½½è§†è§‰éª¨å¹²ç½‘ç»œï¼ˆå›¾åƒéª¨å¹²ï¼‰
            if self.get_vision_tower() is None:
                vision_tower = build_vision_tower(model_args)
                # vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)
                # for k, v in vision_resampler.config.items():
                #     setattr(self.config, k, v)

                if fsdp is not None and len(fsdp) > 0:
                    self.vision_tower = [vision_tower]
                    # self.vision_resampler = [vision_resampler]
                else:
                    self.vision_tower = vision_tower
                    # self.vision_resampler = vision_resampler
            else:
                if fsdp is not None and len(fsdp) > 0:
                    vision_tower = self.vision_tower[0]
                    # vision_resampler = self.vision_resampler[0]
                else:
                    vision_tower = self.vision_tower
                    # vision_resampler = self.vision_resampler
                # if pretrain_vision_modules is not None: # if we has the pretrain model then further delay the loading
                vision_tower.load_model()

            self.config.use_mm_proj = True
            self.config.mm_projector_type = getattr(model_args, "mm_projector_type", "linear")
            # self.config.mm_hidden_size = getattr(vision_resampler, "hidden_size", vision_tower.hidden_size)
            self.config.mm_vision_select_layer = mm_vision_select_layer
            self.config.mm_vision_select_feature = mm_vision_select_feature
            self.config.mm_patch_merge_type = mm_patch_merge_type
            
            # å¦‚æœé…ç½®äº†è¦æ·»åŠ å¿«é€Ÿè§†é¢‘æµçš„tokenï¼Œåˆ™åˆå§‹åŒ–ä¸€ä¸ªå¯å­¦ä¹ çš„ `faster_token`
            if not hasattr(self.config, 'add_faster_video'):
                if hasattr(model_args, 'add_faster_video')  and model_args.add_faster_video:
                    embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))
                    self.faster_token = nn.Parameter(
                        torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std
                    )
            
            # åˆå§‹åŒ–å¤šæ¨¡æ€æŠ•å½±å™¨
            if getattr(self, "mm_projector", None) is None:
                self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)

                if "unpad" in mm_patch_merge_type:
                    embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))
                    self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)
            else:
                # In case it is frozen by LoRA
                # å¦‚æœè¢« LoRA å†»ç»“ï¼Œè§£å†»å‚æ•°
                for p in self.mm_projector.parameters():
                    p.requires_grad = True

            # load the adaptor (åŠ è½½é€‚é…å™¨æƒé‡)
            if pretrain_mm_mlp_adapter is not None:
                mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location="cpu")

                incompatible_keys = self.mm_projector.load_state_dict(get_weight(mm_projector_weights, "mm_projector"))
                print(f"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}")
                incompatible_keys = self.vision_resampler.load_state_dict(get_weight(mm_projector_weights, "vision_resampler"), strict=False)
                print(f"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}")
            
            # ipdb.set_trace()
            # load the vision backbone, adaptor and the self.image_newline
            # åŠ è½½è§†è§‰éª¨å¹²ç½‘ç»œã€é€‚é…å™¨å’Œ image_newline
            if pretrain_vision_modules is not None:
                assert pretrain_mm_mlp_adapter is None, "You give the pretrain_mm_mlp_adapter and pretrain_vision_modules at the same time"
                # load the full model (åŠ è½½å®Œæ•´çš„è§†è§‰æ¨¡å—æƒé‡)
                whole_vision_weights = torch.load(pretrain_vision_modules, map_location="cpu")
                
                # load the backbone #'model.vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight' =>  vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight
                # åŠ è½½éª¨å¹²ç½‘ç»œæƒé‡ï¼Œæ³¨æ„è¿™é‡Œå¯¹æƒé‡é”®ï¼ˆkeyï¼‰è¿›è¡Œäº†å¤„ç†ï¼Œä»¥åŒ¹é…å½“å‰æ¨¡å‹çš„ç»“æ„
                incompatible_keys = self.vision_tower.load_state_dict({'.'.join(k.split(".")[2:]): v for k, v in whole_vision_weights.items() if "vision_tower" in k})
                print(f"ReLoaded vision_tower weights from {pretrain_vision_modules}. Incompatible keys: {incompatible_keys}")
                
                # load the adaptor (åŠ è½½é€‚é…å™¨)
                incompatible_keys = self.mm_projector.load_state_dict(get_weight(whole_vision_weights, "mm_projector"))
                print(f"Loaded mm projector weights from {pretrain_vision_modules}. Incompatible keys: {incompatible_keys}")
                
                # load the newline (åŠ è½½ image_newline æƒé‡)
                self.image_newline.load_state_dict(whole_vision_weights['model.image_newline'])
                print(f'Loaded image_newline weights from {pretrain_vision_modules}.')

        ## handle other config (å¤„ç†å…¶ä»–é…ç½®)
        self.config.mm_newline_position = model_args.mm_newline_position
        self.config.feat_combine_method = model_args.feat_combine_method
        self.config.train_addition_start_end_tokens = model_args.train_addition_start_end_tokens
        
        print(f"å…¶ä»–é…ç½®è®¾ç½®å®Œæˆ: mm_newline_position={self.config.mm_newline_position}, "
                   f"feat_combine_method={self.config.feat_combine_method}, "
                   f"train_addition_start_end_tokens={self.config.train_addition_start_end_tokens}")
        print("è§†è§‰æ¨¡å—åˆå§‹åŒ–å®Œæˆ")

def unpad_image(tensor, original_size):
    """
    Unpads a PyTorch tensor of a padded and resized image.
    å¯¹ä¸€ä¸ªç»è¿‡å¡«å……å’Œç¼©æ”¾çš„å›¾åƒå¼ é‡è¿›è¡Œâ€œå»å¡«å……â€æ“ä½œï¼Œæ¢å¤å…¶åŸå§‹å®½é«˜æ¯”ã€‚

    Args:
    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.
                           å›¾åƒå¼ é‡ï¼Œæ ¼å¼åº”ä¸º CxHxWã€‚
    original_size (tuple): The original size of PIL image (width, height).
                           åŸå§‹ PIL å›¾åƒçš„å°ºå¯¸ (å®½åº¦, é«˜åº¦)ã€‚

    Returns:
    torch.Tensor: The unpadded image tensor.
                  å»å¡«å……åçš„å›¾åƒå¼ é‡ã€‚
    """
    original_width, original_height = original_size
    current_height, current_width = tensor.shape[1:]

    original_aspect_ratio = original_width / original_height
    current_aspect_ratio = current_width / current_height

    if original_aspect_ratio > current_aspect_ratio:
        # å¦‚æœåŸå§‹å›¾åƒæ›´â€œå®½â€ï¼Œè¯´æ˜å¡«å……æ˜¯åœ¨é«˜åº¦ä¸Š
        scale_factor = current_width / original_width
        new_height = int(original_height * scale_factor)
        padding = (current_height - new_height) // 2
        unpadded_tensor = tensor[:, padding:current_height - padding, :]
    else:
        # å¦‚æœåŸå§‹å›¾åƒæ›´â€œé«˜â€ï¼Œè¯´æ˜å¡«å……æ˜¯åœ¨å®½åº¦ä¸Š
        scale_factor = current_height / original_height
        new_width = int(original_width * scale_factor)
        padding = (current_width - new_width) // 2
        unpadded_tensor = tensor[:, :, padding:current_width - padding]

    return unpadded_tensor


class PAVEMetaForCausalLM(ABC):
    """
    ä¸€ä¸ªæŠ½è±¡åŸºç±»ï¼ˆABCï¼‰ï¼Œä¸ºå› æœè¯­è¨€æ¨¡å‹ï¼ˆCausalLMï¼‰é›†æˆ PAVE æ¨¡å‹ã€‚
    å®ƒå®šä¹‰äº†å¤„ç†å¤šæ¨¡æ€è¾“å…¥çš„é€šç”¨æ¥å£å’Œæ ¸å¿ƒé€»è¾‘ã€‚
    """

    @abstractmethod
    def get_model(self):
        """æŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼Œç”¨äºè¿”å›åº•å±‚çš„è¯­è¨€æ¨¡å‹å®ä¾‹ã€‚"""
        pass

    def get_video_tower(self):
        """è·å–è§†é¢‘å¡”ã€‚"""
        return self.get_model().get_video_tower()

    def get_vision_tower(self):
        """è·å–è§†è§‰å¡”ã€‚"""
        return self.get_model().get_vision_tower()

    def encode_videos(self, 
                      video_feats, 
                      q_text_embeds=None, 
                      video_feat_fps=None, 
                      feat_frame_nums=None,
                      q_text_nums=None,
                      chunk_num=None,
                      slow_feats=None):
        """
        ç¼–ç è§†é¢‘ç‰¹å¾ã€‚è¿™æ˜¯"å¿«é€Ÿè·¯å¾„"çš„æ ¸å¿ƒã€‚
        """
        print("å¼€å§‹ç¼–ç è§†é¢‘ç‰¹å¾ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰")
        print(f"è¾“å…¥å½¢çŠ¶: video_feats={video_feats.shape if video_feats is not None else None}, "
                    f"feat_frame_nums={feat_frame_nums}, chunk_num={chunk_num}")
        
        # Using the question text embedding in the diffusion module of the OpenSora
        # we add one extra layer in temporal_aggregator to keep all trainable params in temporal_aggregator
        # å¦‚æœæ—¶é—´èšåˆå™¨ä¸­åŒ…å« diffusion_mlpï¼Œåˆ™ä½¿ç”¨é—®é¢˜æ–‡æœ¬åµŒå…¥ä½œä¸º diffusion æ¨¡å—çš„æ¡ä»¶è¾“å…¥
        if hasattr(self.get_model().temporal_aggregator, 'diffusion_mlp') and self.get_model().temporal_aggregator.diffusion_mlp is not None:
            print("æ£€æµ‹åˆ°diffusion_mlpï¼Œä½¿ç”¨é—®é¢˜åµŒå…¥ä½œä¸ºæ¡ä»¶è¾“å…¥")
            assert self.get_model().get_video_tower().opensora_diffusion is not None , "The MLP defined but the diffusion is not used"
            # å°†é—®é¢˜åµŒå…¥è½¬æ¢ä¸º MLP æœŸæœ›çš„æ•°æ®ç±»å‹
            q_text_embeds = q_text_embeds.to(dtype=self.get_model().temporal_aggregator.diffusion_mlp.layers[0].weight.dtype)
            # é€šè¿‡ MLP å¤„ç†é—®é¢˜åµŒå…¥
            diffusion_text_embedding = self.get_model().temporal_aggregator.diffusion_mlp(q_text_embeds)
            print(f"diffusionæ–‡æœ¬åµŒå…¥å½¢çŠ¶: {diffusion_text_embedding.shape}")
        else:
            print("æœªä½¿ç”¨diffusionæ¡ä»¶è¾“å…¥")
            diffusion_text_embedding = None

        # encode video feature (ç¼–ç è§†é¢‘ç‰¹å¾)
        print("ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„è§†é¢‘ç‰¹å¾ï¼ˆè·³è¿‡è§†é¢‘å¡”ç¼–ç ï¼‰")
        video_features = video_feats
        new_frame_num = feat_frame_nums        
        # è°ƒæ•´ç»´åº¦é¡ºåºä»¥åŒ¹é…æ—¶é—´èšåˆå™¨çš„è¾“å…¥è¦æ±‚
        video_features = video_features.permute([0,2,3,4,1]) # [B, C, T, H, W] -> [B, T, H, W, C]
        
        # **è¿™é‡Œæ˜¯ä½¿ç”¨ temporal_aggregator å¤„ç†ç‰¹å¾çš„æ ¸å¿ƒä½ç½®**
        if hasattr(self.get_model().temporal_aggregator, 'use_slow_as_query') and self.get_model().temporal_aggregator.use_slow_as_query:
            print("æ—¶é—´èšåˆå™¨ä½¿ç”¨æ…¢é€Ÿç‰¹å¾ä½œä¸ºæŸ¥è¯¢")
            video_features = self.get_model().temporal_aggregator(video_features, new_frame_num, 
                                                                q_text_embeds=q_text_embeds, 
                                                                q_text_nums=q_text_nums,
                                                                chunk_num=chunk_num,
                                                                slow_feats=slow_feats)  # ğŸ”¥ æ ¸å¿ƒå¤„ç†ä½ç½®1
        else:
            print("æ—¶é—´èšåˆå™¨æ­£å¸¸æ¨¡å¼ï¼ˆä¸ä½¿ç”¨æ…¢é€Ÿç‰¹å¾ä½œä¸ºæŸ¥è¯¢ï¼‰")
            video_features = self.get_model().temporal_aggregator(video_features, new_frame_num, 
                                                                q_text_embeds=q_text_embeds, 
                                                                q_text_nums=q_text_nums,
                                                                chunk_num=chunk_num)  # ğŸ”¥ æ ¸å¿ƒå¤„ç†ä½ç½®2
        
        # handle output feature number for the case with the cross-attn 
        print("å¤„ç†æ—¶é—´èšåˆå™¨è¾“å‡ºçš„tokenæ•°é‡")
        if hasattr(self.get_model().temporal_aggregator, 'use_query_tokens') and self.get_model().temporal_aggregator.use_query_tokens: # v2 version
            print(f"V2ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šæŸ¥è¯¢tokenæ•°({self.get_model().temporal_aggregator.num_query_tokens})")
            new_frame_num = torch.tensor([self.get_model().temporal_aggregator.num_query_tokens] * video_features.shape[0]).to(video_features.device)
        if type(self.get_model().temporal_aggregator).__name__ == 'SSMTemporalAggregatorV4': # for v4 version, although this may not neccessary
            print(f"V4ç‰ˆæœ¬ï¼štokenæ•°ä¸ºæŸ¥è¯¢æ•°Ã—chunkæ•°({self.get_model().temporal_aggregator.number_of_query}Ã—{chunk_num})")
            new_frame_num = torch.tensor([self.get_model().temporal_aggregator.number_of_query*chunk_num] * video_features.shape[0]).to(video_features.device)
        
        print(f"è§†é¢‘ç¼–ç å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {video_features.shape}, æ–°å¸§æ•°: {new_frame_num}")
        return video_features, new_frame_num

    def encode_images(self, images, return_feat_before_mlp=False):
        """ç¼–ç å›¾åƒã€‚è¿™æ˜¯"æ…¢é€Ÿè·¯å¾„"çš„æ ¸å¿ƒã€‚"""
        print("å¼€å§‹ç¼–ç å›¾åƒç‰¹å¾ï¼ˆæ…¢é€Ÿè·¯å¾„ï¼‰")
        print(f"è¾“å…¥å›¾åƒå½¢çŠ¶: {images.shape}, return_feat_before_mlp: {return_feat_before_mlp}")
        
        # ä½¿ç”¨è§†è§‰å¡”æå–ç‰¹å¾
        image_features_before_mlp = self.get_model().get_vision_tower()(images)
        print(f"è§†è§‰å¡”è¾“å‡ºç‰¹å¾å½¢çŠ¶: {image_features_before_mlp.shape}")
        
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®ï¼Œä»¥ä¾¿è¿›è¡Œåç»­è®¡ç®—
        image_features_before_mlp = image_features_before_mlp.to(dtype=self.dtype) # update the data type for eval
        # å°†æå–çš„ç‰¹å¾é€šè¿‡å¤šæ¨¡æ€æŠ•å½±å™¨ï¼ˆmm_projectorï¼‰ï¼Œå¯¹é½åˆ°è¯­è¨€æ¨¡å‹ç©ºé—´
        image_features = self.get_model().mm_projector(image_features_before_mlp)
        print(f"æŠ•å½±å™¨è¾“å‡ºç‰¹å¾å½¢çŠ¶: {image_features.shape}")
        
        if return_feat_before_mlp:
            print("è¿”å›æŠ•å½±å‰åçš„ç‰¹å¾")
            return image_features, image_features_before_mlp
        else:
            print("åªè¿”å›æŠ•å½±åçš„ç‰¹å¾")
            return image_features

    def get_2dPool(self, image_feature, stride=2):
        """å¯¹å›¾åƒç‰¹å¾è¿›è¡Œ 2D æ± åŒ–æ“ä½œï¼Œä»¥å‡å°‘ç©ºé—´ç»´åº¦ï¼ˆtoken æ•°é‡ï¼‰ã€‚"""
        print(f"å¼€å§‹2Dæ± åŒ–ï¼Œè¾“å…¥å½¢çŠ¶: {image_feature.shape}, stride: {stride}")
        
        height = width = self.get_vision_tower().num_patches_per_side # è·å– patch çš„è¾¹é•¿
        num_frames, num_tokens, num_dim = image_feature.shape
        print(f"æ± åŒ–å‚æ•°: height={height}, width={width}, frames={num_frames}")
        
        # å°†æ‰å¹³åŒ–çš„ patch ç‰¹å¾æ¢å¤ä¸º 2D ç½‘æ ¼å½¢çŠ¶
        image_feature = image_feature.view(num_frames, height, width, -1)
        # è°ƒæ•´ç»´åº¦é¡ºåºä»¥é€‚åº” PyTorch çš„æ± åŒ–å‡½æ•° (N, C, H, W)
        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()
        
        pool_mode = self.config.mm_spatial_pool_mode
        print(f"ä½¿ç”¨{pool_mode}æ± åŒ–æ¨¡å¼")
        
        if pool_mode == "average":
            image_feature = nn.functional.avg_pool2d(image_feature, stride)
        elif pool_mode == "max":
            image_feature = nn.functional.max_pool2d(image_feature, stride)
        elif pool_mode == "bilinear":
            # ä½¿ç”¨åŒçº¿æ€§æ’å€¼è¿›è¡Œä¸‹é‡‡æ ·
            height, weight = image_feature.shape[2:]
            scaled_shape = [math.ceil(height / stride), math.ceil(weight / stride)]
            print(f"åŒçº¿æ€§æ’å€¼ç›®æ ‡å½¢çŠ¶: {scaled_shape}")
            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')
        else:
            raise ValueError(f"Unexpected mm_spatial_pool_mode: {pool_mode}")
        
        # å°†ç»´åº¦é¡ºåºæ¢å¤ï¼Œå¹¶å†æ¬¡æ‰å¹³åŒ–
        image_feature = image_feature.permute(0, 2, 3, 1)
        image_feature = image_feature.view(num_frames, -1, num_dim)
        
        print(f"2Dæ± åŒ–å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {image_feature.shape}")
        return image_feature

    def add_token_per_grid(self, image_feature):
        """ä¸ºæ¯ä¸ªç½‘æ ¼ï¼ˆgridï¼‰æ·»åŠ ä¸€ä¸ªç‰¹æ®Š tokenï¼ˆå¦‚æ¢è¡Œç¬¦ï¼‰ã€‚"""
        resize_h = int(math.sqrt(image_feature.shape[1])) # è®¡ç®—ç½‘æ ¼çš„é«˜åº¦
        num_frames = image_feature.shape[0]
        feature_dim = image_feature.shape[-1]
        
        # æ¢å¤ç½‘æ ¼å½¢çŠ¶
        image_feature = image_feature.view(num_frames, 1, resize_h, resize_h, -1)
        image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()
        image_feature = image_feature.flatten(1, 2).flatten(2, 3)
        # åœ¨æ¯ä¸ªç½‘æ ¼çš„æœ«å°¾æ‹¼æ¥ä¸Š image_newline token
        image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)
        # ... (æ³¨é‡Šæ‰çš„ä»£ç å—) ...
        # æ‰å¹³åŒ–å¹¶è°ƒæ•´ç»´åº¦é¡ºåºï¼Œè¿”å›æœ€ç»ˆç»“æœ
        image_feature = image_feature.flatten(1, 2).transpose(0, 1)
        return image_feature

    def add_token_per_frame(self, image_feature):
        """ä¸ºæ¯ä¸€å¸§ï¼ˆframeï¼‰æ·»åŠ ä¸€ä¸ªç‰¹æ®Š tokenã€‚"""
        # è°ƒæ•´ç»´åº¦é¡ºåº
        image_feature = image_feature.permute(2, 0, 1).contiguous()
        # åœ¨æ¯å¸§ç‰¹å¾çš„æœ«å°¾æ‹¼æ¥ä¸Š image_newline token
        image_feature =  torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)
        # æ¢å¤åŸå§‹ç»´åº¦é¡ºåº
        image_feature = image_feature.permute(1, 2, 0).contiguous()
        return image_feature

    def prepare_image_features(self, images, image_sizes, modalities, return_feat_before_mlp=False):
        '''
            This function is for encode the image feature.
            è¯¥å‡½æ•°ç”¨äºç¼–ç å›¾åƒç‰¹å¾ã€‚
            images: list[tensor]: shape of the tensor is torch.Size([32, 3, 384, 384]), len of list = batchsize
                                  ä¸€ä¸ªå¼ é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡ä»£è¡¨ä¸€æ‰¹å¸§ï¼Œå½¢çŠ¶ä¸º [å¸§æ•°, é€šé“, é«˜, å®½]
            image_sizes: list[int]: represent the H*W*C of the origin video frames
                                    åŸå§‹è§†é¢‘å¸§çš„å°ºå¯¸ä¿¡æ¯
            modalities: list[string]: string should be "video"
                                     æ¨¡æ€ç±»å‹åˆ—è¡¨ï¼ŒæŒ‡ç¤ºæ¯ä¸ªè¾“å…¥æ˜¯ "video" è¿˜æ˜¯ "image"
        '''
        print("å¼€å§‹å‡†å¤‡å›¾åƒç‰¹å¾")
        print(f"è¾“å…¥å‚æ•°: imagesç±»å‹={type(images)}, image_sizes={image_sizes}, modalities={modalities}")
        
        if images is None: # We do not have image as input (å¦‚æœæ²¡æœ‰å›¾åƒè¾“å…¥ï¼Œç›´æ¥è¿”å› None)
            print("æ²¡æœ‰å›¾åƒè¾“å…¥ï¼Œè¿”å›None")
            return None
        
        # import pdb; pdb.set_trace()
        if type(images) is list or images.ndim == 5:
            if type(images) is list:
                print("è¾“å…¥ä¸ºå›¾åƒåˆ—è¡¨ï¼Œç¡®ä¿æ¯ä¸ªå…ƒç´ è‡³å°‘æ˜¯4ç»´")
                # ç¡®ä¿æ¯ä¸ªå›¾åƒ/è§†é¢‘å¸§å¼ é‡è‡³å°‘æ˜¯4ç»´çš„
                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]

            # æ‰¾å‡ºæ‰¹æ¬¡ä¸­å“ªäº›æ˜¯è§†é¢‘
            video_idx_in_batch = []
            for idx, modality in enumerate(modalities):
                if modality == "video":
                    video_idx_in_batch.append(idx)
            print(f"æ‰¹æ¬¡ä¸­è§†é¢‘ç´¢å¼•: {video_idx_in_batch}")

            images_list = []
            for image in images:
                if image.ndim == 4:
                    images_list.append(image)
                else:
                    images_list.append(image.unsqueeze(0))

            # å°†æ‰€æœ‰å›¾åƒ/å¸§æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„æ‰¹æ¬¡ï¼Œä»¥ä¾¿ä¸€æ¬¡æ€§é€å…¥ç¼–ç å™¨
            concat_images = torch.cat([image for image in images_list], dim=0) # e.g., torch.Size([64, 3, 384, 384])
            # è®°å½•æ¯ä¸ªåŸå§‹å›¾åƒ/è§†é¢‘çš„å¸§æ•°ï¼Œä»¥ä¾¿åç»­æ‹†åˆ†
            split_sizes = [image.shape[0] for image in images_list] # e.g., [32, 32]
            print(f"æ‹¼æ¥åå›¾åƒå½¢çŠ¶: {concat_images.shape}, æ‹†åˆ†å¤§å°: {split_sizes}")
            
            if return_feat_before_mlp:
                print("ç¼–ç å›¾åƒç‰¹å¾ï¼ˆè¿”å›æŠ•å½±å‰åç‰¹å¾ï¼‰")
                # ç¼–ç æ‹¼æ¥åçš„å¤§æ‰¹æ¬¡å›¾åƒï¼Œå¹¶è·å–æŠ•å½±å‰åçš„ç‰¹å¾
                encoded_image_features, encoded_image_features_before_mlp = self.encode_images(concat_images, return_feat_before_mlp=return_feat_before_mlp)
                
                # å°†ç¼–ç åçš„ç‰¹å¾æŒ‰åŸå§‹å¤§å°æ‹†åˆ†å›åˆ—è¡¨
                encoded_image_features = torch.split(encoded_image_features, split_sizes) 
                encoded_image_features_before_mlp = torch.split(encoded_image_features_before_mlp, split_sizes) 
                image_features = []
                image_features_before_mlp = []
                # éå†æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾
                for idx, (image_feat, image_feat_before_mlp) in enumerate(zip(encoded_image_features, encoded_image_features_before_mlp)):
                    if idx in video_idx_in_batch: # å¦‚æœæ˜¯è§†é¢‘
                        print(f"æ ·æœ¬{idx}æ˜¯è§†é¢‘ï¼Œåº”ç”¨2Dæ± åŒ–")
                        # å¯¹è§†é¢‘å¸§ç‰¹å¾è¿›è¡Œ 2D æ± åŒ–ï¼Œå‡å°‘ token æ•°é‡
                        image_features.append(self.get_2dPool(image_feat))
                        image_features_before_mlp.append(self.get_2dPool(image_feat_before_mlp))
                    else: # å¦‚æœæ˜¯å•å¼ å›¾åƒ
                        print(f"æ ·æœ¬{idx}æ˜¯å›¾åƒï¼Œç›´æ¥ä½¿ç”¨ç‰¹å¾")
                        image_features.append(image_feat)
                        image_features_before_mlp.append(image_feat_before_mlp)
                
                print("å›¾åƒç‰¹å¾å‡†å¤‡å®Œæˆï¼ˆåŒ…å«æŠ•å½±å‰ç‰¹å¾ï¼‰")
                return image_features, video_idx_in_batch, image_features_before_mlp
            else:
                print("ç¼–ç å›¾åƒç‰¹å¾ï¼ˆä»…è¿”å›æŠ•å½±åç‰¹å¾ï¼‰")
                # åªè·å–æŠ•å½±åçš„ç‰¹å¾
                encoded_image_features = self.encode_images(concat_images, return_feat_before_mlp=return_feat_before_mlp)
                
                # å°†ç¼–ç åçš„ç‰¹å¾æŒ‰åŸå§‹å¤§å°æ‹†åˆ†
                encoded_image_features = torch.split(encoded_image_features, split_sizes) 
                image_features = []
                for idx, image_feat in enumerate(encoded_image_features):
                    if idx in video_idx_in_batch: # å¦‚æœæ˜¯è§†é¢‘
                        print(f"æ ·æœ¬{idx}æ˜¯è§†é¢‘ï¼Œåº”ç”¨2Dæ± åŒ–")
                        # åº”ç”¨ 2D æ± åŒ–
                        image_features.append(self.get_2dPool(image_feat))
                    else: # å¦‚æœæ˜¯å›¾åƒ
                        print(f"æ ·æœ¬{idx}æ˜¯å›¾åƒï¼Œç›´æ¥ä½¿ç”¨ç‰¹å¾")
                        image_features.append(image_feat)
                
                print("å›¾åƒç‰¹å¾å‡†å¤‡å®Œæˆï¼ˆä»…æŠ•å½±åç‰¹å¾ï¼‰")
                return image_features, video_idx_in_batch
        else:
            print("ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼")
            raise NotImplementedError

    def post_processing_of_image_feature(self, image_features, video_idx_in_batch):
        '''
            This function is for some post-processing of the image feature, 
            like flatten and adding special tokens
            æ­¤å‡½æ•°ç”¨äºå›¾åƒç‰¹å¾çš„åå¤„ç†ï¼Œä¾‹å¦‚æ‰å¹³åŒ–å’Œæ·»åŠ ç‰¹æ®Š tokenã€‚
        '''
        print("å¼€å§‹å›¾åƒç‰¹å¾åå¤„ç†")
        
        mm_patch_merge_type = getattr(self.config, "mm_patch_merge_type", "flat") # 'spatial_unpad'
        image_aspect_ratio = getattr(self.config, "image_aspect_ratio", "square") # 'anyres_max_9'
        mm_newline_position = getattr(self.config, "mm_newline_position", "one_token") # image:'one_token', video: 'no_token'
        
        print(f"åå¤„ç†é…ç½®: patch_merge_type={mm_patch_merge_type}, "
                    f"aspect_ratio={image_aspect_ratio}, newline_position={mm_newline_position}")

        if mm_patch_merge_type == "flat":
            print("ä½¿ç”¨flatæ¨¡å¼ï¼šç›´æ¥æ‰å¹³åŒ–å¸§å’Œpatchç»´åº¦")
            # ç›´æ¥å°†å¸§å’Œ patch ç»´åº¦æ‰å¹³åŒ–
            image_features = [x.flatten(0, 1) for x in image_features]

        elif mm_patch_merge_type.startswith("spatial"): # INTO HERE (è¿›å…¥æ­¤åˆ†æ”¯)
            print("ä½¿ç”¨spatialæ¨¡å¼è¿›è¡Œåå¤„ç†")
            new_image_features = []
            for image_idx, image_feature in enumerate(image_features):
                if image_idx in video_idx_in_batch:  # video operations (è§†é¢‘æ“ä½œ)
                    print(f"å¤„ç†è§†é¢‘{image_idx}ï¼Œnewline_position={mm_newline_position}")
                    
                    if mm_newline_position == "grid":
                        print("æŒ‰ç½‘æ ¼æ·»åŠ token")
                        # Grid-wise: æŒ‰ç½‘æ ¼æ·»åŠ  token
                        image_feature = self.add_token_per_grid(image_feature)
                        new_image_features.append(image_feature)
                    elif mm_newline_position == "frame":
                        print("æŒ‰å¸§æ·»åŠ token")
                        # Frame-wise: æŒ‰å¸§æ·»åŠ  token
                        image_feature = self.add_token_per_frame(image_feature) # e.g., [32, 169, 896] -> [32, 170, 896]
                        new_image_features.append(image_feature.flatten(0, 1)) # æ‰å¹³åŒ–
                    elif mm_newline_position == "one_token":
                        print("æ·»åŠ å•ä¸ªtoken")
                        # one-token: åªåœ¨æ‰€æœ‰ç‰¹å¾çš„æœ€åæ·»åŠ ä¸€ä¸ª token
                        image_feature = image_feature.flatten(0, 1)
                        if 'unpad' in mm_patch_merge_type:
                            image_feature = torch.cat((
                                image_feature,
                                self.model.image_newline[None].to(image_feature.device)
                            ), dim=0)
                        new_image_features.append(image_feature)      
                    elif mm_newline_position == "no_token":
                        print("ä¸æ·»åŠ ç‰¹æ®Štokenï¼Œç›´æ¥æ‰å¹³åŒ–")
                        # ä¸æ·»åŠ ä»»ä½•ç‰¹æ®Š tokenï¼Œç›´æ¥æ‰å¹³åŒ–
                        new_image_features.append(image_feature.flatten(0, 1))
                    else:
                        print(f"ä¸æ”¯æŒçš„newline_position: {mm_newline_position}")
                        raise ValueError(f"Unexpected mm_newline_position: {mm_newline_position}")
                else:  # single image operations (å•å›¾æ“ä½œ)
                    print(f"å¤„ç†å•å›¾{image_idx}ï¼ˆæœªå®ç°ï¼‰")
                    raise NotImplementedError
            image_features = new_image_features
        else:
            print(f"ä¸æ”¯æŒçš„patch_merge_type: {mm_patch_merge_type}")
            raise ValueError(f"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}")
        
        print("å›¾åƒç‰¹å¾åå¤„ç†å®Œæˆ")
        return image_features

    def prepare_inputs_labels_for_multimodal(
                self,
                input_ids,
                position_ids,
                attention_mask,
                past_key_values,
                labels,
                video_feats,
                video_feat_fps=None,
                feat_frame_nums=None,
                question_ids=None,
                question_lens=None,
                # for the image frames (ç”¨äºå›¾åƒå¸§çš„å‚æ•°)
                images=None,
                image_sizes=None,
                modalities=None,
                video_metas=None,
            ):
        """
        æ ¸å¿ƒå‡½æ•°ï¼šå‡†å¤‡å¤šæ¨¡æ€è¾“å…¥çš„ input_embeds å’Œ labelsã€‚
        å®ƒå°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç‰¹å¾æ•´åˆåœ¨ä¸€èµ·ï¼Œç”Ÿæˆæœ€ç»ˆé€å…¥è¯­è¨€æ¨¡å‹çš„è¾“å…¥ã€‚
        
        attention_maskçš„ä½œç”¨ï¼š
        1. æ ‡è¯†å“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆtokenï¼Œå“ªäº›æ˜¯padding
        2. åœ¨å¤šæ¨¡æ€èåˆåé‡æ–°è®¡ç®—maskï¼Œç¡®ä¿è§†è§‰tokenè¢«æ­£ç¡®å…³æ³¨
        3. é˜²æ­¢æ¨¡å‹å…³æ³¨åˆ°paddingä½ç½®ï¼Œæé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡
        """
        print("å¼€å§‹å‡†å¤‡å¤šæ¨¡æ€è¾“å…¥")
        print(f"è¾“å…¥å‚æ•°: input_ids.shape={input_ids.shape if input_ids is not None else None}, "
                    f"attention_mask.shape={attention_mask.shape if attention_mask is not None else None}, "
                    f"video_feats.shape={video_feats.shape if video_feats is not None else None}, "
                    f"imagesç±»å‹={type(images)}, modalities={modalities}")
        
        video_tower = self.get_video_tower()
        vision_tower = self.get_vision_tower()
        
        # å¦‚æœæ²¡æœ‰è§†è§‰æ¨¡å—ï¼Œæˆ–è€…æ²¡æœ‰è§†è§‰è¾“å…¥ï¼Œæˆ–è€…åœ¨æ¨ç†ï¼ˆåªè¾“å…¥ä¸€ä¸ª tokenï¼‰ï¼Œåˆ™ç›´æ¥è¿”å›æ–‡æœ¬è¾“å…¥
        if (video_tower is None and vision_tower is None) or (video_feats is None and images is None) or input_ids.shape[1] == 1:
            print("è·³è¿‡å¤šæ¨¡æ€å¤„ç†ï¼šæ— è§†è§‰æ¨¡å—æˆ–è§†è§‰è¾“å…¥ï¼Œæˆ–å¤„äºæ¨ç†æ¨¡å¼")
            return input_ids, position_ids, attention_mask, past_key_values, None, labels
        
        # embed the question id using our LLM
        # ä½¿ç”¨ LLM çš„ token_embedding å±‚å°†é—®é¢˜ ID è½¬æ¢ä¸ºåµŒå…¥
        if question_ids is not None:
            print("å°†é—®é¢˜IDè½¬æ¢ä¸ºåµŒå…¥")
            question_embeds = self.get_model().embed_tokens(question_ids).detach()
        else:
            print("æ— é—®é¢˜IDè¾“å…¥")
            question_embeds = None
        
        # figure out the chunk size (ç¡®å®šè§†é¢‘å—çš„æ•°é‡)
        if images is not None:
            chunk_num = images[0].shape[0]
            print(f"æ£€æµ‹åˆ°chunkæ•°é‡: {chunk_num}")
        else:
            chunk_num = None
            print("æ— chunkä¿¡æ¯")
            
        # special control for the slow feature (å¯¹æ…¢é€Ÿè·¯å¾„ç‰¹å¾çš„ç‰¹æ®Šæ§åˆ¶)
        if hasattr(self.get_model(), 'temporal_aggregator') and getattr(self.get_model().temporal_aggregator, 'use_slow_feat_before_mlp', False): 
            print("å¯ç”¨use_slow_feat_before_mlpæ¨¡å¼")
            assert images is not None and (-1 not in image_sizes) # å¦‚æœä½¿ç”¨ MLP å‰çš„æ…¢é€Ÿç‰¹å¾ï¼Œå½“å‰åªæ”¯æŒåŸå§‹è§†é¢‘è¾“å…¥
            self.use_slow_feat_before_mlp = True
        else:
            print("æœªå¯ç”¨use_slow_feat_before_mlpæ¨¡å¼")
            self.use_slow_feat_before_mlp = False

        # get the image feature (The slow feature) (è·å–å›¾åƒç‰¹å¾ï¼Œå³æ…¢é€Ÿç‰¹å¾)
        if images is not None and (-1 not in image_sizes):
            print("å¤„ç†åŸå§‹å›¾åƒè¾“å…¥")
            if self.use_slow_feat_before_mlp:
                # å‡†å¤‡å›¾åƒç‰¹å¾ï¼Œå¹¶è¿”å›æŠ•å½±å‰åçš„ç‰¹å¾
                image_features, video_idx_in_batch, image_features_before_mlp = self.prepare_image_features(images, image_sizes, modalities, return_feat_before_mlp=self.use_slow_feat_before_mlp)
            else:
                # åªå‡†å¤‡æŠ•å½±åçš„ç‰¹å¾
                image_features, video_idx_in_batch = self.prepare_image_features(images, image_sizes, modalities)
        else: # the image feature is loaded (å›¾åƒç‰¹å¾æ˜¯é¢„å…ˆåŠ è½½çš„)
            print("ä½¿ç”¨é¢„åŠ è½½çš„å›¾åƒç‰¹å¾")
            assert sum(image_sizes) == -len(image_sizes) # ç¡®è®¤æ‰€æœ‰ image_sizes éƒ½æ˜¯ -1
            image_features = images
            video_idx_in_batch = []
            for idx, modality in enumerate(modalities):
                if modality == "video":
                    video_idx_in_batch.append(idx)
        
        # get the video feature (The fast feature) (è·å–è§†é¢‘ç‰¹å¾ï¼Œå³å¿«é€Ÿç‰¹å¾)
        if video_feats is None:
            print("æ— è§†é¢‘ç‰¹å¾è¾“å…¥")
            video_features, new_frame_num = None, None
        else:
            print("ç¼–ç è§†é¢‘ç‰¹å¾")
            video_features, new_frame_num = self.encode_videos(video_feats, 
                                                            q_text_embeds=question_embeds,
                                                            video_feat_fps=video_feat_fps, 
                                                            feat_frame_nums=feat_frame_nums,
                                                            q_text_nums=question_lens,
                                                            chunk_num=chunk_num,
                                                            slow_feats=image_features if not self.use_slow_feat_before_mlp else image_features_before_mlp)

        # add up the video and image features. (åˆå¹¶è§†é¢‘å’Œå›¾åƒç‰¹å¾)
        feat_combine_method = getattr(self.config, 'feat_combine_method', 'concat')
        print(f"ä½¿ç”¨ç‰¹å¾åˆå¹¶æ–¹æ³•: {feat_combine_method}")
        
        if video_features is not None and feat_combine_method == 'add':
            print("ä½¿ç”¨addæ–¹æ³•åˆå¹¶å¿«é€Ÿå’Œæ…¢é€Ÿç‰¹å¾")
            # å¦‚æœæ˜¯ 'add' æ–¹æ³•ï¼Œå°†å¿«é€Ÿç‰¹å¾å’Œæ…¢é€Ÿç‰¹å¾ç›¸åŠ 
            assert image_features[0].shape[1]*image_features[0].shape[0] == video_features.shape[1]
            
            updated_image_feat = []
            for curr_video_feat, curr_image_feat in zip(video_features, image_features):
                curr_video_feat = rearrange(curr_video_feat, "(k s) d -> k s d", k=chunk_num)  # è°ƒæ•´å¿«é€Ÿç‰¹å¾å½¢çŠ¶ä»¥åŒ¹é…æ…¢é€Ÿç‰¹å¾
                updated_image_feat.append(curr_video_feat + curr_image_feat)
            image_features = updated_image_feat
        
        # proprocessing of the image feature (å›¾åƒç‰¹å¾çš„åå¤„ç†)
        if images is not None:
            print("å¯¹å›¾åƒç‰¹å¾è¿›è¡Œåå¤„ç†")
            image_features = self.post_processing_of_image_feature(image_features, video_idx_in_batch)
        
        # Combine image and video feature, and update the new_frame_num (åˆå¹¶å›¾åƒå’Œè§†é¢‘ç‰¹å¾ï¼Œå¹¶æ›´æ–°å¸§æ•°)
        train_addition_start_end_tokens = getattr(self.config, 'train_addition_start_end_tokens', False)
        print(f"train_addition_start_end_tokens: {train_addition_start_end_tokens}")
        
        if train_addition_start_end_tokens:
            assert self.get_model().temporal_aggregator.start_end_tokens is not None
            start_end_token_set = self.get_model().temporal_aggregator.start_end_tokens
        
        if video_features is not None:
            if feat_combine_method == 'concat':
                print("ä½¿ç”¨concatæ–¹æ³•åˆå¹¶å¿«é€Ÿå’Œæ…¢é€Ÿç‰¹å¾")
                # 'concat' æ–¹æ³•ï¼šå°†æ…¢é€Ÿç‰¹å¾å’Œå¿«é€Ÿç‰¹å¾åœ¨ token ç»´åº¦ä¸Šæ‹¼æ¥
                image_features = torch.cat([ele.unsqueeze(dim=0) for ele in image_features], dim=0)
                video_features = torch.cat([image_features, video_features], dim=-2)
                new_frame_num += image_features.shape[1]
            elif feat_combine_method == 'interleave':
                print("ä½¿ç”¨interleaveæ–¹æ³•äº¤é”™åˆå¹¶ç‰¹å¾")
                # 'interleave' æ–¹æ³•ï¼šäº¤é”™åˆå¹¶å¿«é€Ÿå’Œæ…¢é€Ÿç‰¹å¾
                interleaved_feat = []
                interleaved_frame_num = []
                frame_number_per_video = images[0].shape[0] # æ¯æ®µè§†é¢‘çš„å¸§æ•°
                print(f"æ¯æ®µè§†é¢‘å¸§æ•°: {frame_number_per_video}")
                
                for curr_img_feat, curr_vid_feat, curr_vid_feat_len in zip(image_features, video_features, new_frame_num):
                    #### handle the image features (å¤„ç†å›¾åƒç‰¹å¾)
                    total_image_tokens = curr_img_feat.shape[0]
                    assert total_image_tokens % frame_number_per_video == 0
                    tokens_per_frame = total_image_tokens // frame_number_per_video
                    image_feat_split_sizes = [tokens_per_frame for i in range(frame_number_per_video)]
                    splited_image_feat = torch.split(curr_img_feat, image_feat_split_sizes) # æŒ‰å¸§æ‹†åˆ†æ…¢é€Ÿç‰¹å¾
                    
                    #### handle the video features (å¤„ç†è§†é¢‘ç‰¹å¾)
                    updated_video_feat = curr_vid_feat[:curr_vid_feat_len]
                    video_feat_split_sizes = split_list_lengths(curr_vid_feat_len, frame_number_per_video)
                    splited_video_feat = torch.split(updated_video_feat, video_feat_split_sizes) # æŒ‰å¸§æ‹†åˆ†å¿«é€Ÿç‰¹å¾
                    
                    #### combine the feature (åˆå¹¶ç‰¹å¾)
                    combined_feat = []
                    for i_f, v_f in zip(splited_image_feat, splited_video_feat):
                        if train_addition_start_end_tokens:
                            print("æ·»åŠ èµ·æ­¢token")
                            # å¦‚æœä½¿ç”¨é¢å¤–çš„èµ·æ­¢ tokenï¼ŒæŒ‰ç‰¹å®šé¡ºåºæ‹¼æ¥
                            combined_feat.append(start_end_token_set[0].unsqueeze(dim=0))
                            combined_feat.append(v_f)
                            combined_feat.append(start_end_token_set[1].unsqueeze(dim=0))
                            combined_feat.append(start_end_token_set[2].unsqueeze(dim=0))
                            combined_feat.append(i_f)
                            combined_feat.append(start_end_token_set[3].unsqueeze(dim=0))
                        else:
                            # å¦åˆ™ç›´æ¥æ‹¼æ¥å¿«é€Ÿå’Œæ…¢é€Ÿç‰¹å¾
                            combined_feat.append(v_f)
                            combined_feat.append(i_f)
                    combined_feat = torch.cat(combined_feat)
                    interleaved_feat.append(combined_feat)
                    interleaved_frame_num.append(combined_feat.shape[0])
                new_frame_num = interleaved_frame_num
                video_features = interleaved_feat
            elif feat_combine_method == 'add':
                print("addæ–¹æ³•å·²åœ¨å‰é¢å¤„ç†")
                # 'add' æ–¹æ³•å·²åœ¨å‰é¢å¤„ç†ï¼Œè¿™é‡Œæ›´æ–°å¸§æ•°
                video_features = image_features
                new_frame_num = [ele.shape[0] for ele in image_features]
            else:
                print(f"ä¸æ”¯æŒçš„ç‰¹å¾åˆå¹¶æ–¹æ³•: {feat_combine_method}")
                raise NotImplementedError
        else: # IF we are not using the fast path (å¦‚æœä¸ä½¿ç”¨å¿«é€Ÿè·¯å¾„)
            print("ä¸ä½¿ç”¨å¿«é€Ÿè·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨æ…¢é€Ÿç‰¹å¾")
            video_features = image_features
            for ele in video_features:
                ele.requires_grad = True # ç¡®ä¿ç‰¹å¾æœ‰æ¢¯åº¦
            new_frame_num = [ele.shape[0] for ele in image_features]
        
        print("å¼€å§‹æ„å»ºæœ€ç»ˆçš„è¾“å…¥åºåˆ—")
        
        # Let's just add dummy tensors if they do not exist...
        # ä¸º None çš„å¼ é‡åˆ›å»ºè™šæ‹Ÿå¼ é‡ï¼Œä»¥ç®€åŒ–åç»­å¤„ç†
        _labels = labels
        _position_ids = position_ids
        _attention_mask = attention_mask
        
        print("å¤„ç†attention_mask:")
        if attention_mask is None:
            print("attention_maskä¸ºNoneï¼Œæ ¹æ®input_idsåˆ›å»ºå…¨1çš„mask")
            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
            print(f"åˆ›å»ºçš„attention_maskå½¢çŠ¶: {attention_mask.shape}")
        else:
            print(f"ä½¿ç”¨è¾“å…¥çš„attention_maskï¼Œå½¢çŠ¶: {attention_mask.shape}")
            attention_mask = attention_mask.bool()
        
        print("attention_maskçš„ä½œç”¨è§£é‡Š:")
        print("1. åŸå§‹ä½œç”¨ï¼šæ ‡è¯†input_idsä¸­å“ªäº›ä½ç½®æ˜¯çœŸå®token(1)ï¼Œå“ªäº›æ˜¯padding(0)")
        print("2. åœ¨æ–‡æœ¬å¤„ç†ä¸­ï¼šé˜²æ­¢æ¨¡å‹å…³æ³¨paddingä½ç½®")
        print("3. åœ¨å¤šæ¨¡æ€å¤„ç†ä¸­ï¼šéœ€è¦é‡æ–°è®¡ç®—ä»¥åŒ…å«è§†è§‰tokençš„ä½ç½®")
        
        if position_ids is None:
            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)
        if labels is None:
            labels = torch.full_like(input_ids, IGNORE_INDEX)

        # remove the padding using attention_mask -- FIXME
        # ä½¿ç”¨ attention_mask å»é™¤å¡«å……éƒ¨åˆ†ï¼Œå¾—åˆ°å¯å˜é•¿åº¦çš„åºåˆ—
        print("ä½¿ç”¨attention_maskå»é™¤paddingï¼Œè·å¾—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆåºåˆ—:")
        _input_ids = input_ids
        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]
        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]
        
        # æ‰“å°å»é™¤paddingåçš„åºåˆ—é•¿åº¦
        for i, (ids, mask) in enumerate(zip(input_ids, [cur_attention_mask for cur_attention_mask in attention_mask])):
            print(f"æ ·æœ¬{i}: åŸå§‹é•¿åº¦={mask.shape[0]}, æœ‰æ•ˆé•¿åº¦={ids.shape[0]}, paddingæ¯”ä¾‹={1-ids.shape[0]/mask.shape[0]:.2%}")

        new_input_embeds = []
        new_labels = []
        cur_image_idx = 0
        # éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªæ ·æœ¬
        for batch_idx, cur_input_ids in enumerate(input_ids):
            # determine how many image we have (ç¡®å®šå½“å‰æ ·æœ¬æœ‰å¤šå°‘ä¸ªå›¾åƒ/è§†é¢‘å ä½ç¬¦)
            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()
            if num_images == 0:
                # å¦‚æœæ²¡æœ‰å›¾åƒï¼Œç›´æ¥å¤„ç†æ–‡æœ¬ï¼ˆè™½ç„¶è¿™é‡Œçš„é€»è¾‘ä¼¼ä¹ä¸å®Œæ•´ï¼Œcur_image_features[0:0] æ˜¯ç©ºå¼ é‡ï¼‰
                cur_image_features = video_features[cur_image_idx]
                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)
                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)
                new_input_embeds.append(cur_input_embeds)
                new_labels.append(labels[batch_idx])
                cur_image_idx += 1
                continue

            # æ‰¾åˆ°æ‰€æœ‰å›¾åƒå ä½ç¬¦çš„ä½ç½®
            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]
            cur_input_ids_noim = []
            cur_labels = labels[batch_idx]
            cur_labels_noim = []
            # æ ¹æ®å›¾åƒå ä½ç¬¦çš„ä½ç½®ï¼Œå°†æ–‡æœ¬å’Œæ ‡ç­¾åˆ‡åˆ†æˆæ®µ
            for i in range(len(image_token_indices) - 1):
                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])
                cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])
            split_sizes = [x.shape[0] for x in cur_labels_noim]
            
            # å°†åˆ‡åˆ†åçš„æ–‡æœ¬ ID è½¬æ¢ä¸ºåµŒå…¥
            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))
            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)
            cur_new_input_embeds = []
            cur_new_labels = []

            # merge the text embedding with the image embedding
            # å°†æ–‡æœ¬åµŒå…¥å’Œè§†è§‰ç‰¹å¾åµŒå…¥äº¤é”™åˆå¹¶
            for i in range(num_images + 1):
                # æ·»åŠ ä¸€æ®µæ–‡æœ¬åµŒå…¥
                cur_new_input_embeds.append(cur_input_embeds_no_im[i])
                cur_new_labels.append(cur_labels_noim[i])
                if i < num_images:
                    # æ·»åŠ ä¸€æ®µè§†è§‰ç‰¹å¾åµŒå…¥
                    cur_image_features = video_features[cur_image_idx]
                    cur_feature_len = new_frame_num[cur_image_idx]
                    cur_image_features = cur_image_features[:cur_feature_len] # (T, C) or (T, H*W, C)
                    if len(cur_image_features.shape) == 3:
                        cur_image_features = cur_image_features.view(-1, cur_image_features.shape[-1])
                    cur_image_idx += 1
                    cur_new_input_embeds.append(cur_image_features)
                    # è§†è§‰ç‰¹å¾å¯¹åº”çš„æ ‡ç­¾åº”è¢«å¿½ç•¥ï¼Œä¸è®¡å…¥æŸå¤±
                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))

            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]
            
            # å°†æ‰€æœ‰ç‰‡æ®µæ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„åºåˆ—
            cur_new_input_embeds = torch.cat(cur_new_input_embeds)
            cur_new_labels = torch.cat(cur_new_labels)

            new_input_embeds.append(cur_new_input_embeds)
            new_labels.append(cur_new_labels)

        # Truncate sequences to max length as image embeddings can make the sequence longer
        # å› ä¸ºåŠ å…¥äº†è§†è§‰ç‰¹å¾ï¼Œåºåˆ—å¯èƒ½å˜å¾—å¾ˆé•¿ï¼Œéœ€è¦æˆªæ–­åˆ°æ¨¡å‹èƒ½æ¥å—çš„æœ€å¤§é•¿åº¦
        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)
        if tokenizer_model_max_length is not None:
            print(f"æˆªæ–­åºåˆ—åˆ°æœ€å¤§é•¿åº¦: {tokenizer_model_max_length}")
            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]
            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]

        # Combine them by padding
        # å°†æ‰¹æ¬¡å†…æ‰€æœ‰å¯å˜é•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒçš„æœ€å¤§é•¿åº¦
        max_len = max(x.shape[0] for x in new_input_embeds)
        batch_size = len(new_input_embeds)
        
        print(f"é‡æ–°æ„å»ºattention_maskç”¨äºå¤šæ¨¡æ€è¾“å…¥:")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, æœ€å¤§åºåˆ—é•¿åº¦: {max_len}")
        
        # åˆå§‹åŒ–å¡«å……åçš„å¼ é‡
        new_input_embeds_padded = []
        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)
        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)
        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)
        
        print("é‡æ–°æ„å»ºçš„attention_maskä½œç”¨:")
        print("1. æ–°çš„maskæ ‡è¯†èåˆååºåˆ—ä¸­çš„æœ‰æ•ˆä½ç½®ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œè§†è§‰tokenï¼‰")
        print("2. 0è¡¨ç¤ºpaddingä½ç½®ï¼Œ1è¡¨ç¤ºæœ‰æ•ˆçš„tokenä½ç½®")
        print("3. ç¡®ä¿Transformeråªå…³æ³¨æœ‰æ„ä¹‰çš„æ–‡æœ¬å’Œè§†è§‰å†…å®¹")
        
        # éå†æ¯ä¸ªæ ·æœ¬å¹¶è¿›è¡Œå¡«å……
        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):
            cur_len = cur_new_embed.shape[0]
            padding_side = getattr(self.config, 'tokenizer_padding_side', 'right')
            
            print(f"æ ·æœ¬{i}: æœ‰æ•ˆé•¿åº¦={cur_len}, éœ€è¦paddingé•¿åº¦={max_len-cur_len}, paddingæ–¹å‘={padding_side}")
            
            if padding_side == "left":
                # å·¦å¡«å……
                print(f"  æ‰§è¡Œå·¦å¡«å……ï¼Œæœ‰æ•ˆtokenä½ç½®: [{max_len-cur_len}:{max_len}]")
                new_input_embeds_padded.append(torch.cat((
                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),
                    cur_new_embed
                ), dim=0))
                if cur_len > 0:
                    new_labels_padded[i, -cur_len:] = cur_new_labels
                    attention_mask[i, -cur_len:] = True  # å·¦å¡«å……æ—¶ï¼Œå³ä¾§ä¸ºæœ‰æ•ˆä½ç½®
                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)
            else:
                # å³å¡«å…… (é»˜è®¤)
                print(f"  æ‰§è¡Œå³å¡«å……ï¼Œæœ‰æ•ˆtokenä½ç½®: [0:{cur_len}]")
                new_input_embeds_padded.append(torch.cat((
                    cur_new_embed,
                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)
                ), dim=0))
                if cur_len > 0:
                    new_labels_padded[i, :cur_len] = cur_new_labels
                    attention_mask[i, :cur_len] = True  # å³å¡«å……æ—¶ï¼Œå·¦ä¾§ä¸ºæœ‰æ•ˆä½ç½®
                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)

        # å°†åˆ—è¡¨å †å æˆä¸€ä¸ªæ‰¹æ¬¡å¼ é‡
        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)

        print(f"æœ€ç»ˆattention_maskç»Ÿè®¡:")
        for i in range(batch_size):
            valid_tokens = attention_mask[i].sum().item()
            print(f"  æ ·æœ¬{i}: æœ‰æ•ˆtokenæ•°={valid_tokens}/{max_len}, æ¯”ä¾‹={valid_tokens/max_len:.2%}")

        # match the output with the input (åŒ¹é…è¾“å‡ºæ ¼å¼å’ŒåŸå§‹è¾“å…¥)
        if _labels is None:
            new_labels = None
        else:
            new_labels = new_labels_padded

        if _attention_mask is None:
            print("åŸå§‹attention_maskä¸ºNoneï¼Œè¿”å›None")
            attention_mask = None
        else:
            print("è½¬æ¢attention_maskæ•°æ®ç±»å‹ä»¥åŒ¹é…åŸå§‹è¾“å…¥")
            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)

        if _position_ids is None:
            position_ids = None

        print("attention_maskå¤„ç†å®Œæˆï¼Œæ€»ç»“å…¶å…³é”®ä½œç”¨:")
        print("1. è¾“å…¥é˜¶æ®µï¼šè¯†åˆ«åŸå§‹æ–‡æœ¬åºåˆ—ä¸­çš„æœ‰æ•ˆtoken")
        print("2. å¤„ç†é˜¶æ®µï¼šç”¨äºå»é™¤paddingï¼Œè·å¾—çœŸå®çš„å¯å˜é•¿åº¦åºåˆ—")
        print("3. èåˆé˜¶æ®µï¼šé‡æ–°è®¡ç®—ä»¥åŒ…å«æ’å…¥çš„è§†è§‰tokenä½ç½®")
        print("4. è¾“å‡ºé˜¶æ®µï¼šæŒ‡å¯¼Transformeræ¨¡å‹æ­£ç¡®æ‰§è¡Œattentionè®¡ç®—")
        print("5. æœ€ç»ˆæ•ˆæœï¼šç¡®ä¿æ¨¡å‹åªå…³æ³¨æœ‰æ„ä¹‰çš„æ–‡æœ¬å’Œè§†è§‰å†…å®¹ï¼Œå¿½ç•¥padding")
        
        # æœ€ç»ˆè¿”å›çš„æ˜¯å¤„ç†å¥½çš„ embedding å’Œç›¸å…³å¼ é‡ï¼Œinput_ids è¿”å› Noneï¼Œå› ä¸ºå·²ç»è¢« embedding æ›¿ä»£
        print("å¤šæ¨¡æ€è¾“å…¥å‡†å¤‡å®Œæˆ")
        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels

    def initialize_vision_tokenizer(self, model_args, tokenizer):
        """åˆå§‹åŒ–ä¸è§†è§‰ç›¸å…³çš„ tokenizer è®¾ç½®ï¼Œä¸»è¦æ˜¯æ·»åŠ æ–°çš„ç‰¹æ®Š tokenã€‚"""
        print("å¼€å§‹åˆå§‹åŒ–è§†è§‰tokenizer")
        
        # use additional image token between the image and the text
        if model_args.mm_use_im_patch_token:
            print("æ·»åŠ å›¾åƒpatch token")
            # æ·»åŠ è¡¨ç¤ºå›¾åƒ patch çš„ token
            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)
            self.resize_token_embeddings(len(tokenizer))

        if model_args.mm_use_im_start_end:
            print("æ·»åŠ å›¾åƒ/è§†é¢‘å¼€å§‹å’Œç»“æŸtoken")
            # æ·»åŠ è¡¨ç¤ºå›¾åƒ/è§†é¢‘å¼€å§‹å’Œç»“æŸçš„ token
            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)
            self.resize_token_embeddings(len(tokenizer)) # è°ƒæ•´ embedding çŸ©é˜µå¤§å°ä»¥å®¹çº³æ–° token

            if num_new_tokens > 0:
                print(f"æ·»åŠ äº†{num_new_tokens}ä¸ªæ–°tokenï¼Œåˆå§‹åŒ–å…¶embedding")
                # å°†æ–° token çš„ embedding åˆå§‹åŒ–ä¸ºæ‰€æœ‰æ—§ token embedding çš„å¹³å‡å€¼
                input_embeddings_avg = self.get_input_embeddings().weight.data[:-num_new_tokens].mean(
                    dim=0, keepdim=True)
                output_embeddings_avg = self.get_input_embeddings().weight.data[:-num_new_tokens].mean(
                    dim=0, keepdim=True)

                self.get_input_embeddings().weight.data[-num_new_tokens:] = input_embeddings_avg
                self.get_input_embeddings().weight.data[-num_new_tokens:] = output_embeddings_avg # Bug? Should be output_embeddings_avg?

            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦éœ€è¦è®­ç»ƒè¿™äº›æ–°çš„ token embedding
            if model_args.tune_temporal_aggregator or model_args.tune_addition_token_embeddings:
                print("è§£å†»æ–°tokençš„embeddingå‚æ•°")
                for p in self.get_input_embeddings().parameters():
                    p.requires_grad = True # è§£å†»è¾“å…¥ embedding å±‚
                for p in self.get_output_embeddings().parameters():
                    p.requires_grad = False # ä¿æŒè¾“å‡º embedding å±‚ï¼ˆè¯­è¨€æ¨¡å‹å¤´ï¼‰å†»ç»“

            if model_args.pretrain_temporal_aggregator:
                print("ä»é¢„è®­ç»ƒæƒé‡åŠ è½½æ–°tokençš„embedding")
                # å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼Œåˆ™åŠ è½½æ–° token çš„ embedding
                mm_projector_weights = torch.load(model_args.pretrain_temporal_aggregator, map_location='cpu')
                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']
                assert num_new_tokens == 2
                if self.get_input_embeddings().weight.data.shape == embed_tokens_weight.shape:
                    self.get_input_embeddings().weight.data[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]
                elif embed_tokens_weight.shape[0] == num_new_tokens:
                    self.get_input_embeddings().weight.data[-num_new_tokens:] = embed_tokens_weight
                else:
                    raise ValueError(f"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {self.get_input_embeddings().weight.data.shape}. Numer of new tokens: {num_new_tokens}.")
        
        elif model_args.mm_use_im_patch_token:
            # freeze the input and output tokens
            # å¦‚æœåªä½¿ç”¨ patch token è€Œä¸ä½¿ç”¨ start/end tokenï¼Œä¸”éœ€è¦å¾®è°ƒèšåˆå™¨ï¼Œåˆ™å†»ç»“ token embedding
            if model_args.tune_temporal_aggregator:
                print("å†»ç»“è¾“å…¥å’Œè¾“å‡ºtoken embedding")
                self.get_input_embeddings().requires_grad_(False)
                self.get_output_embeddings().requires_grad_(False)       

        ### handle the special case which the len(tokenizer) != self.get_input_embeddings().weight.data.shape[0]
        # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼štokenizer çš„è¯æ±‡è¡¨å¤§å°ä¸ embedding çŸ©é˜µå¤§å°ä¸ä¸€è‡´ï¼Œè¿›è¡Œè°ƒæ•´
        if len(tokenizer) != self.get_input_embeddings().weight.data.shape[0]:
            print(f"Tokenizerè¯æ±‡è¡¨å¤§å°({len(tokenizer)})ä¸embeddingçŸ©é˜µå¤§å°({self.get_input_embeddings().weight.data.shape[0]})ä¸ä¸€è‡´ï¼Œè¿›è¡Œè°ƒæ•´")
            self.resize_token_embeddings(len(tokenizer))
        
        print("è§†è§‰tokenizeråˆå§‹åŒ–å®Œæˆ")