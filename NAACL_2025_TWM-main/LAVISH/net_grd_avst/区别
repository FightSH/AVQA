1.main_avst.py 使用 AVQA_Fusion_Net (定义在 net_avst.py):
    这是一个端到端的音视问答模型。
    它内部集成了问题编码器 (QstEncoder)、视觉网络 (resnet18)、音频特征处理层、以及复杂的音视融合机制，包括注意力模块 (attn_a, attn_v) 和多层感知机 (fc1 到 fc4 用于匹配，fc_ans 用于问答)。
    此模型旨在直接从音频、视觉（正负样本）和问题输入中预测答案，并进行音视匹配判断。
2.main_alvs.py 使用 AVQA_Net (定义在 net_alvs.py):
这个模型更侧重于学习对齐的音频和视觉（或视觉和文本）表示，通常用于对比学习任务。
    AVQA_Net 内部包含：
        visual_encoder: 这是一个 AVQA_Fusion_Net 的实例，但它来自于 net_encoder.py。这个版本的 AVQA_Fusion_Net (在 net_encoder.py 中定义) 与 net_avst.py 中的版本在结构上相似，但在其 forward 方法中，它主要用于提取视觉特征 (f_v) 和问题特征 (qst_feature)，而不进行完整的问答和匹配计算。
        audio_encoder: 这是一个 AMS (Audio Multi-Scale) 模型，定义在 net_encoder.py，用于处理音频特征并与视觉特征交互。
        visual_mapping: 一个线性层，用于映射视觉特征。
    AVQA_Net 的目标是根据不同的 mode ("visual" 或 "audio") 输出相应的特征，这些特征随后用于计算对比损失 (如 InfoNCE)。



前向过程 (Forward Pass) 的不同
1.AVQA_Fusion_Net
   输入: audio, visual_posi (正面视觉样本), visual_nega (负面视觉样本), question。
    处理流程:
        1.问题文本通过 question_encoder 转换为 qst_feature。
        2.音频数据通过 fc_a1, fc_a2 提取 audio_feat。
        3.对 visual_posi 进行处理：
            提取全局视觉特征 (visual_feat_before_grounding_posi)。
            执行音视定位 (grounding) 得到 visual_feat_grd_after_grounding_posi。
            融合两者并通过 fc_gl 得到 visual_feat_grd_posi。
            将 audio_feat 和 visual_feat_grd_posi 串联，通过一系列全连接层 (fc1 到 fc4) 得到正面匹配分数 out_match_posi。
        4.对 visual_nega 进行类似处理，得到负面匹配分数 out_match_nega。
        5.将 qst_feature 作为查询，分别对 visual_feat_grd_posi (处理后的正面视觉特征) 和 audio_feat_pure (处理后的音频特征) 应用多头注意力 (attn_v, attn_a)。
        6.融合注意力机制后的音视特征，并与 qst_feature 进一步融合。
        7.最终通过 fc_ans 输出问答预测 out_qa。
    输出: out_qa (问答结果), out_match_posi (正面匹配结果), out_match_nega (负面匹配结果)。


2.AVQA_Net
    输入: audio, visual_posi, question。
    处理流程:
        1.visual_encoder (即 net_encoder.AVQA_Fusion_Net) 处理 audio 和 visual_posi。注意，这个 visual_encoder 的 forward 方法 (定义在 net_encoder.py) 只返回初步的视觉特征 f_v 和问题特征 qst_feature (如果问题被处理)。
        2.得到的 f_v 通过 self.visual_mapping 线性层。
        3.根据 self.mode 参数进行分支：
            如果 mode == "visual":
                问题特征 f_qst 通过 self.visual_encoder.qst_feature_forward(question) 计算得到。
                音频特征 f_a 设置为 None。
            如果 mode != "visual" (例如 "audio"):
                音频特征 f_a 通过 self.audio_encoder(audio_re, f_v) 计算得到，这里 audio_encoder 是 AMS 模型，它会结合原始音频和先前提取的视觉特征 f_v。
                问题特征 f_qst 设置为 None。
    输出: f_v (处理后的视觉特征), f_a (处理后的音频特征，或 None), f_qst (处理后的问题特征，或 None)。这些特征主要用于后续的对比损失计算。


损失函数区别

main_avst.py (用于音视问答和匹配任务):
    在 train 函数中，它使用了 nn.CrossEntropyLoss() 作为 criterion。
这个损失函数被用于两个方面：
    问答损失 (loss_qa): loss_qa = criterion(out_qa, target)，计算模型预测答案 (out_qa)与真实答案 (target) 之间的交叉熵损失。
    匹配损失 (loss_match): loss_match = criterion(out_match, match_label)，计算模型对音视样本对（正面和负面）的匹配预测 (out_match)与真实匹配标签 (match_label) 之间的交叉熵损失。
总损失: loss = loss_qa + 0.5 * loss_match，最终的损失是问答损失和匹配损失的加权和（匹配损失的权重是0.5）。


main_alvs.py (用于对比学习的表示学习任务):
    它定义并使用了自定义的 InfoNCELoss 类。
    InfoNCELoss 内部也是基于交叉熵损失，但其目标是最大化正样本对之间的相似性，并最小化负样本对之间的相似性。
    在 train 函数中：
        criterion = InfoNCELoss()
        根据 mode 参数的不同，损失计算方式也不同：
            如果 mode == "visual":
            loss_v = criterion(f_v, f_qst) (视觉特征与问题特征的对比损失)
            loss_q = criterion(f_qst, f_v) (问题特征与视觉特征的对比损失)
            loss = loss_q + loss_v
            如果 mode != "visual" (例如 "audio"):
            loss_v = criterion(f_v, f_a) (视觉特征与音频特征的对比损失)
            loss_a = criterion(f_a, f_v) (音频特征与视觉特征的对比损失)
            loss = loss_a + loss_v
    InfoNCELoss 的 forward 方法计算了两个方向的对比损失（例如，u对v 和 v对u），然后将它们相加。


